{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a067d5",
   "metadata": {},
   "source": [
    "This notebook was used to practice with the spaCy library in order to use it effectively in the final project of Data Processing.\n",
    "\n",
    "Name: Julia Wervers\n",
    "Student number: 13168665"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbc78d7",
   "metadata": {},
   "source": [
    "Chapter 1: Finding words, phrases, names and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install spacy\n",
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd99d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "\n",
    "# Create a blank English nlp object\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03036ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n",
      "I\n",
      "was\n",
      "soon\n",
      "introduced\n",
      "into\n",
      "the\n",
      "presence\n",
      "of\n",
      "the\n",
      "magistrate\n",
      ",\n",
      "an\n",
      "old\n",
      "benevolent\n",
      "man\n",
      ",\n",
      "with\n",
      "calm\n",
      "and\n",
      "mild\n",
      "manners\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello world!\")\n",
    "\n",
    "# Iterate over tokens in a Doc\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "#test\n",
    "my_text = 'I was soon introduced into the presence of the magistrate, an old benevolent man, with calm and mild manners.'\n",
    "doc = nlp(my_text)\n",
    "#Iterate over tokens\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5601bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soon\n",
      "an old benevolent man\n"
     ]
    }
   ],
   "source": [
    "#Try out indexing\n",
    "my_token = doc[2]\n",
    "print(my_token.text)\n",
    "\n",
    "my_span = doc[11:15]\n",
    "print(my_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9076c23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_alpha: [True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, False, True, True, True, True, True, False]\n",
      "is_punct: [False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, True, False, False, False, False, False, True]\n",
      "like_num: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "#Test out token attributes\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09deda68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON\n",
      "was AUX\n",
      "soon ADV\n",
      "introduced VERB\n",
      "into ADP\n",
      "the DET\n",
      "presence NOUN\n",
      "of ADP\n",
      "the DET\n",
      "magistrate NOUN\n",
      ", PUNCT\n",
      "an DET\n",
      "old ADJ\n",
      "benevolent ADJ\n",
      "man NOUN\n",
      ", PUNCT\n",
      "with ADP\n",
      "calm ADJ\n",
      "and CCONJ\n",
      "mild ADJ\n",
      "manners NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "#Trained pipelines\n",
    "#from spacy.cli import download\n",
    "#download(\"en_core_web_sm\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Could help me filter out the words with tags I'm interested in\n",
    "# Iterate over the tokens\n",
    "my_text = 'I was soon introduced into the presence of the magistrate, an old benevolent man, with calm and mild manners.'\n",
    "doc = nlp(my_text)\n",
    "for token in doc:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfbb232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubjpass introduced\n",
      "was AUX auxpass introduced\n",
      "soon ADV advmod introduced\n",
      "introduced VERB ROOT introduced\n",
      "into ADP prep introduced\n",
      "the DET det presence\n",
      "presence NOUN pobj into\n",
      "of ADP prep presence\n",
      "the DET det magistrate\n",
      "magistrate NOUN pobj of\n",
      ", PUNCT punct magistrate\n",
      "an DET det man\n",
      "old ADJ amod man\n",
      "benevolent ADJ amod man\n",
      "man NOUN appos magistrate\n",
      ", PUNCT punct presence\n",
      "with ADP prep presence\n",
      "calm ADJ amod manners\n",
      "and CCONJ cc calm\n",
      "mild ADJ conj calm\n",
      "manners NOUN pobj with\n",
      ". PUNCT punct introduced\n"
     ]
    }
   ],
   "source": [
    "#Tells us about relations between words\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "700a3097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monetary values, including unit'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Helps us find entities in the text -> probably not super useful for my project, but very cool!\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "#As expected there's no output, as there aren't entities in the sample text that I am using. \n",
    "\n",
    "#The following function is very helpful, though, in the case we do find entities in the text.\n",
    "spacy.explain(\"MONEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e14ebedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benevolent man\n"
     ]
    }
   ],
   "source": [
    "#Very interesting and cool method, but not sure if it will be useful for the project\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"benevolent\"}, {\"TEXT\": \"man\"}]\n",
    "matcher.add(\"MAN_PATTERN\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "468dd228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "was\n",
      "soon\n",
      "introduced\n",
      "into\n",
      "the\n",
      "presence\n",
      "of\n",
      "the\n",
      "magistrate\n",
      "an\n",
      "old\n",
      "benevolent\n",
      "man\n",
      "with\n",
      "calm\n",
      "and\n",
      "mild\n",
      "manners\n"
     ]
    }
   ],
   "source": [
    "#Actually might be helpful for filtering text ?\n",
    "pattern = [\n",
    "    {\"IS_PUNCT\": False},\n",
    "]\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"FILTER_PATTERN\", [pattern])\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)\n",
    "    \n",
    "#Doesn't seem to take \"IS_DET\": False and \"IS_CCONJ\": False as attributes though, which is a bummer\n",
    "#Make sure to reinitialize the matcher, as otherwise previously filter patterns will also be applied, here leading to another mention\n",
    "#of benevolent man before reinitilizaing the matcher."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05574d7e",
   "metadata": {},
   "source": [
    "Chapter 2: Large-scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7c90e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 3197928453018144401\n",
      "string value: coffee\n",
      "hash value: 3197928453018144401\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"coffee\"])\n",
    "print(\"string value:\", nlp.vocab.strings[3197928453018144401])\n",
    "\n",
    "print(\"hash value:\", doc.vocab.strings[\"coffee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f8f71cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coffee 3197928453018144401 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I love coffee\")\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# Print the lexical attributes\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30a94f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "#Good to know but probably not necessary for my project\n",
    "# Create an nlp object\n",
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Import the Doc class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f5ad0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "#Good to know but probably not necessary for my project\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "\n",
    "print(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6ee98035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.869833325851152\n",
      "0.30932289361953735\n",
      "0.18213694934365615\n",
      "0.4719003666806404\n"
     ]
    }
   ],
   "source": [
    "#This could be very useful for my final report. I could use this to compare the list of key words once I obtain those!!\n",
    "\n",
    "#from spacy.cli import download\n",
    "#download(\"en_core_web_md\")\n",
    "\n",
    "# Load a larger pipeline with vectors\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))\n",
    "\n",
    "\n",
    "#Can also do it for tokens\n",
    "token1 = doc1[2]\n",
    "token2 = doc1[3]\n",
    "print(token1.similarity(token2))\n",
    "\n",
    "#Can also compare documents to one token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))\n",
    "\n",
    "# And we can compare spans with documents\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d39cef9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
